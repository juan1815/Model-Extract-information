{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers import BertTokenizer, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas = {\n",
    "    'O' : 0,\n",
    "    \"FECHA\" : 1,\n",
    "    \"DIRECTOR\" : 2,\n",
    "    \"ROL_ENTITY\": 3,\n",
    "    \"EDICION\" : 4,\n",
    "    \"TIPO_DOCUMENTO\" : 5,\n",
    "    \"ROMAN_NUM\" : 6,\n",
    "    \"NUM_ISSN\" : 7,\n",
    "    \"ENTITY\": 8,\n",
    "    \"PRESENTATION\": 9\n",
    "}\n",
    "\n",
    "# load model and tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', num_labels=len(etiquetas))\n",
    "model = BertForTokenClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', num_labels=len(etiquetas))\n",
    "\n",
    "# Move the model to the GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZAR AND CODIFY THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_codify(phrase, tokenizer, tags):\n",
    "    tokens_codified = tokenizer(\n",
    "        [token for token, tag in phrase],\n",
    "        is_split_into_words=True,  # True if the input is already tokenized\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    tags_codified = torch.tensor([tags[tag] for token, tag in phrase])\n",
    "    return tokens_codified, tags_codified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [\n",
    "    [(\"Bogotá\", \"O\"), (\"D.C.\", \"O\"), (\",\", \"O\"), (\"jueves\", \"O\"), (\",\", \"O\"), (\"13\", \"FECHA\"), (\"de\", \"O\"), (\"diciembre\", \"FECHA\"), (\"de\", \"O\"), (\"2019\", \"FECHA\")],\n",
    "    [(\"GREGORIO\", \"DIRECTOR\"), (\"ELJACH\", \"DIRECTOR\"), (\"PACHECO\", \"DIRECTOR\"), (\"Y\", \"O\"), (\"MARIA\", \"DIRECTOR\"), (\"GOMEZ\", \"DIRECTOR\"), (\".\", \"O\")],\n",
    "    [(\"DIRECORES\", \"DIRECTOR\"), (\"SECRETARIO\", \"ROL_ENTITY\"), (\"GENERAL\", \"ROL_ENTITY\"), (\"DE\", \"O\"), (\"LA\", \"O\"), (\"CÁMARA\", \"ROL_ENTITY\")],\n",
    "    [(\"GREGORIO\", \"DIRECTOR\"), (\"ELJACH\", \"DIRECTOR\"), (\"PACHECO\", \"DIRECTOR\"), (\"Y\", \"O\"), (\"MARIA\", \"DIRECTOR\"), (\"GOMEZ\", \"DIRECTOR\"), (\".\", \"O\")],\n",
    "    [(\"DIRECORES\", \"DIRECTOR\"), (\"SECRETARIO\", \"ROL_ENTITY\"), (\"GENERAL\", \"ROL_ENTITY\"), (\"DEL\", \"O\"), (\"SENADO\", \"ROL_ENTITY\")]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    4, 17521,  1090,  1009,  1059,  1009,  1017, 11573,  1017,  2083,\n",
      "          1008,  2451,  1008, 17736,     5]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "tokens_codified, tags_codified = tokenizar_codify(data_train[0], tokenizer, etiquetas)\n",
    "print(tokens_codified)\n",
    "print(tags_codified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este resultado muestra la arquitectura del modelo BERT que se ha cargado, incluyendo las capas de embeddings, atención, y clasificación. La advertencia inicial indica que algunas de las capas del clasificador (classifier.bias y classifier.weight) no se han inicializado desde el punto de control del modelo y se han inicializado de nuevo. Esto es normal cuando se carga un modelo preentrenado y se ajusta para una tarea específica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = \"model_bert_ner.pt_gacetas\"\n",
    "ruta_modelo = os.path.join(\"modelos\", name_model)\n",
    "\n",
    "# --- Existing the pro\n",
    "os.makedirs(\"modelos\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_codify(phrase, tokenizer, tags):\n",
    "    tokens = [token for token, tag in phrase]\n",
    "    tags_list = [tags[tag] for token, tag in phrase]\n",
    "    \n",
    "    tokens_codified = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,  # True if the input is already tokenized\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # Align the tags with the tokenized input\n",
    "    word_ids = tokens_codified.word_ids()\n",
    "    tags_codified = []\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            tags_codified.append(-100)  # Special token\n",
    "        else:\n",
    "            tags_codified.append(tags_list[word_id])\n",
    "    \n",
    "    tags_codified = torch.tensor(tags_codified)\n",
    "    return tokens_codified, tags_codified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example training data and tags dictionary\n",
    "data_train = [\n",
    "    [(\"Hello\", \"O\"), (\"world\", \"O\")],\n",
    "    [(\"My\", \"O\"), (\"name\", \"O\"), (\"is\", \"O\"), (\"BERT\", \"B-PER\")]\n",
    "]\n",
    "etiquetas = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-LOC\": 3,\n",
    "    \"I-LOC\": 4,\n",
    "    \"B-ORG\": 5,\n",
    "    \"I-ORG\": 6,\n",
    "    \"B-MISC\": 7,\n",
    "    \"I-MISC\": 8\n",
    "}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize and codify the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokens_codified, tags_codified \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizar_codify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metiquetas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Debug prints to check lengths\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of input_ids: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokens_codified[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 14\u001b[0m, in \u001b[0;36mtokenizar_codify\u001b[1;34m(phrase, tokenizer, tags)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokens_codified \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m      6\u001b[0m     tokens,\n\u001b[0;32m      7\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# True if the input is already tokenized\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Align the tags with the tokenized input\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokens_codified\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m tags_codified \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_id \u001b[38;5;129;01min\u001b[39;00m word_ids:\n",
      "File \u001b[1;32mc:\\Users\\Jorge\\anaconda3\\envs\\Extract\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:398\u001b[0m, in \u001b[0;36mBatchEncoding.word_ids\u001b[1;34m(self, batch_index)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03mReturn a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m    (several tokens will be mapped to the same word index if they are parts of that word).\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[batch_index]\u001b[38;5;241m.\u001b[39mword_ids\n",
      "\u001b[1;31mValueError\u001b[0m: word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class)."
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize and codify the data\n",
    "tokens_codified, tags_codified = tokenizar_codify(data_train[0], tokenizer, etiquetas)\n",
    "\n",
    "# Debug prints to check lengths\n",
    "print(f\"Length of input_ids: {len(tokens_codified['input_ids'][0])}\")\n",
    "print(f\"Length of attention_mask: {len(tokens_codified['attention_mask'][0])}\")\n",
    "print(f\"Length of tags_codified: {len(tags_codified)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure that the tensors have the same length\n",
    "assert len(tokens_codified[\"input_ids\"][0]) == len(tokens_codified[\"attention_mask\"][0]) == len(tags_codified), \"Size mismatch between tensors\"\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(tokens_codified[\"input_ids\"], tokens_codified[\"attention_mask\"], tags_codified)\n",
    "data_loader = DataLoader(dataset, batch_size=8) \n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(etiquetas))\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in tqdm(data_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model and tokenizer\n",
    "ruta_modelo = \"path/to/save/model\"\n",
    "model.save_pretrained(ruta_modelo)\n",
    "tokenizer.save_pretrained(ruta_modelo)\n",
    "\n",
    "print(f\"Modelo guardado en: {ruta_modelo}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
