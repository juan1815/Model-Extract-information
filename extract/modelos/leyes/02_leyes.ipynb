{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import random\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from typing import Optional\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura el logging al principio del script (o en un archivo de configuración separado)\n",
    "logging.basicConfig(level=logging.INFO,  # Nivel de detalle (INFO, DEBUG, WARNING, ERROR, CRITICAL)\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de limpieza OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ocr_issues_for_training(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto para entrenamiento, con manejo de Unicode y regex más precisas.\n",
    "    \"\"\"\n",
    "    # Normalizar caracteres Unicode\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Regex para fechas (más robusta)\n",
    "    text = re.sub(r\"(?P<dia>\\d{1,2})\\s*(?P<mes>ENE|FEB|MAR|ABR|MAY|JUN|JUL|AGO|SEP|OCT|NOV|DIC)\\s*(?P<año>\\d{4})\",\n",
    "                r\"\\g<dia>\\g<mes>\\g<año>\", text, flags=re.IGNORECASE)\n",
    "    # Unir digitos separados en las fechas\n",
    "    text = re.sub(r\"(?P<dia>\\d{1,2})\\s+(?P<mes>ENE|FEB|MAR|ABR|MAY|JUN|JUL|AGO|SEP|OCT|NOV|DIC)\\s+(?P<año>\\d{4})\",\n",
    "            r\"\\g<dia>\\g<mes>\\g<año>\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Quitar saltos de línea extraños dentro de palabras\n",
    "    text = re.sub(r\"(\\w+)\\s+-\\s+(\\w+)\", r\"\\1\\2\", text) #e.g., \"mani-  festación\" -> \"manifestación\"\n",
    "    text = re.sub(r\"(\\w+)\\n(\\w+)\", r\"\\1\\2\", text) #e.g., \"mani\\nfes\" -> \"manifes\"\n",
    "    text = re.sub(r\"(\\w+)-(\\n)(\\w+)\", r\"\\1\\3\", text) #para problema de guion\n",
    "    #Eliminar espacios extras\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[:;,.]+\", r\". \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_list = [\n",
    "    \"\"\"2353 17ABR2024 LEY No. \"POR MEDIO DE LA CUAL...\"\"\",\n",
    "    \"\"\"2355 2024 LEY No. 17MAY \"POR MEDIO DE LA CUAL...\"\"\",\n",
    "    # ...\n",
    "]\n",
    "\n",
    "def add_new_examples(raw_text_list, train_data, output_file=\"train_data.json\", interactive=True):\n",
    "    \"\"\"\n",
    "    Agrega nuevos ejemplos, los anota (manualmente o no) y guarda TRAIN_DATA.\n",
    "\n",
    "    Args:\n",
    "        raw_text_list: Lista de textos crudos.\n",
    "        train_data: Lista de entrenamiento actual (se modifica in-place).\n",
    "        output_file: Nombre del archivo JSON para guardar.\n",
    "        interactive:  Si es True, pide anotación manual. Si es False,\n",
    "                      asume que las entidades ya están en raw_text_list.\n",
    "    \"\"\"\n",
    "    for raw_text in raw_text_list:\n",
    "        cleaned_text = fix_ocr_issues_for_training(raw_text)\n",
    "\n",
    "        if interactive:\n",
    "            print(\"=== Texto limpio para anotar ===\")\n",
    "            print(cleaned_text)\n",
    "            print(\"Longitud:\", len(cleaned_text))\n",
    "\n",
    "            entities = []\n",
    "            while True:\n",
    "                try:\n",
    "                    start = int(input(\"Inicio de entidad (o -1 para terminar): \"))\n",
    "                    if start == -1:\n",
    "                        break\n",
    "                    end = int(input(\"Fin de entidad: \"))\n",
    "                    label = input(\"Etiqueta: \")\n",
    "                    entities.append((start, end, label))\n",
    "                except ValueError:\n",
    "                    print(\"Entrada inválida.  Ingresa números enteros.\")\n",
    "        else:\n",
    "            # Modo no interactivo.  Busca si ya existe y usa sus entidades.\n",
    "            found = False\n",
    "            for i, (existing_text, annotations) in enumerate(train_data):\n",
    "                if existing_text == cleaned_text:\n",
    "                    entities = annotations[\"entities\"]\n",
    "                    found = True\n",
    "                    del train_data[i]  # Lo borra (por si hay duplicados)\n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"WARNING: Texto '{cleaned_text}' no encontrado. Se añade sin entidades.\")\n",
    "                entities = []\n",
    "\n",
    "        train_data.append((cleaned_text, {\"entities\": entities}))\n",
    "\n",
    "    # Guardar TRAIN_DATA en un archivo JSON\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"TRAIN_DATA actualizado guardado en {output_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar TRAIN_DATA: {e}\")\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para cargar TRAIN_DATA desde el archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(input_file=\"train_data.json\"):\n",
    "    \"\"\"Carga los datos de entrenamiento desde un archivo JSON.\"\"\"\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            train_data = json.load(f)\n",
    "            return train_data\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Archivo de datos de entrenamiento no encontrado: {input_file}.  Se usará una lista vacía.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f\"Error al decodificar el archivo JSON: {input_file}.  Verifica el formato.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inesperado al cargar TRAIN_DATA: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 12:37:09,167 - WARNING - Archivo de datos de entrenamiento no encontrado: train_data.json.  Se usará una lista vacía.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Texto limpio para anotar ===\n",
      "8FEB2022 - .  LEY ORGANICA No2199 \"POR MEDIO DE LA CUAL. \" . \n",
      "Longitud: 61\n",
      "Entrada inválida.  Ingresa números enteros.\n"
     ]
    }
   ],
   "source": [
    "# 1. Carga los datos existentes (si los hay)\n",
    "TRAIN_DATA = load_train_data()\n",
    "if not TRAIN_DATA:\n",
    "    TRAIN_DATA = [\n",
    "        (\n",
    "            '2182 3ENE2022 LEY No \"POR MEDIO DEL CUAL SE MODIFICA EL ARTICULO 13 DE LA LEY ) 2002\" EL CONGRESO DE COLOMBIA',\n",
    "            {\"entities\": [\n",
    "                (0, 4, \"NUMERO_LEY\"),   # \"2182\"\n",
    "                (5, 13, \"FECHA\"),      # \"3ENE2022\"\n",
    "                (14, 103, \"EPIGRAFE\") # \"LEY No  POR MEDIO DEL CUAL SE MODIFICA EL ARTICULO 13 DE LA LEY ) 2002\"\n",
    "            ]}\n",
    "        ),\n",
    "        (\n",
    "            '2184 6ENE2022 LEY No. POR MEDIO DE LA CUAL SE DICTAN NORMAS ENCAMINADAS A FOMENTAR, PROMOVER LA SOSTENIBILIDAD, LA VALORACION Y LA TRANSMISION DE LOS SABERES DE LOS OFICIOS ARTISTICOS, DE LAS INDUSTRIAS CREATIVAS Y CULTURALES, ARTESANALES Y DEL PATRIMONIO CULTURAL EN COLOMBIA Y SE DICTAN OTRAS DISPOSICIONES EL CONGRESO DE COLOMBIA',\n",
    "            {\"entities\": [\n",
    "                (0, 4, \"NUMERO_LEY\"),   # \"2184\"\n",
    "                (5, 13, \"FECHA\"),      # \"6ENE2022\"\n",
    "                (14, 236, \"EPIGRAFE\") # \"LEY  No. POR  MEDIO  DE  LA  CUAL  SE  DICTAN  NORMAS  ENCAMINADAS  A  FOMENTAR,  PROMOVER  LA  SOSTENIBILIDAD,  LA  VALORACION  Y  LA  TRANSMISION  DE  LOS  SABERES  DE  LOS  OFICIOS  ARTISTICOS,  DE  LAS  INDUSTRIAS  CREATIVAS  Y  CULTURALES,  ARTESANALES  Y  DEL  PATRIMONIO  CULTURAL  EN  COLOMBIA  Y  SE  DICTAN  OTRAS  DISPOSICIONES\"\n",
    "            ]}\n",
    "        ),\n",
    "        (\n",
    "            '2185 ENE2022 LEY No \"POR MEDIO DE LA CUAL SE CREA EL FESTIVAL NACIONAL DE LA MARIMBA DE CHONTA, Y SE DICTAN OTRAS DISPOSICIONES\" EL CONGRESO DE COLOMBIA',\n",
    "            {\"entities\": [\n",
    "                (0, 4, \"NUMERO_LEY\"),   # \"2185\"\n",
    "                (5, 12, \"FECHA\"),      # \"ENE2022\" (Aquí *no* incluyes el día, o tendrias que modificar tu normalizacion)\n",
    "                (13, 120, \"EPIGRAFE\")   # \"LEY  No  POR  MEDIO  DE  LA  CUAL  SE  CREA  EL  FESTIVAL  NACIONAL DE  LA  MARIMBA  DE  CHONTA,  Y  SE  DICTAN  OTRAS  DISPOSICIONES\"\n",
    "            ]} \n",
    "        ),\n",
    "    ]\n",
    "\n",
    "# *nuevos* ejemplos *manualmente*, usa add_new_examples\n",
    "#    en modo interactivo (sin el `interactive=False`):\n",
    "nuevos_textos = [\n",
    "   \"\"\"8 FEB 2022\n",
    "    - ...\n",
    "    LEY ORGANICA No2199\n",
    "    \"POR MEDIO DE LA CUAL...\" ...\n",
    "    \"\"\",\n",
    "]\n",
    "TRAIN_DATA = add_new_examples(nuevos_textos, TRAIN_DATA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_model(model_path=\"modelo_leyes_epigrafe\"):\n",
    "    return spacy.load(model_path)\n",
    "\n",
    "def normalize_spacy_date(spacy_fecha: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Normaliza fechas y registra errores.\n",
    "    \"\"\"\n",
    "    month_map = {\n",
    "        \"ENE\": \"01\", \"FEB\": \"02\", \"MAR\": \"03\", \"ABR\": \"04\",\n",
    "        \"MAY\": \"05\", \"JUN\": \"06\", \"JUL\": \"07\", \"AGO\": \"08\",\n",
    "        \"SEP\": \"09\", \"OCT\": \"10\", \"NOV\": \"11\", \"DIC\": \"12\"\n",
    "    }\n",
    "\n",
    "    m = re.match(r\"(\\d{1,2})([A-Z]{3,4})(\\d{4})\", spacy_fecha.upper())\n",
    "    if not m:\n",
    "        logging.warning(f\"No se pudo normalizar la fecha: {spacy_fecha}\") # Registro\n",
    "        return None  # O devolver la fecha original, según prefieras\n",
    "    day = m.group(1).zfill(2)\n",
    "    mes_str = m.group(2)\n",
    "    year = m.group(3)\n",
    "    mes_num = month_map.get(mes_str)\n",
    "    if not mes_num:\n",
    "        logging.warning(f\"Mes desconocido en fecha: {spacy_fecha}\") # Registro\n",
    "        return None  # O devolver la fecha original\n",
    "    return f\"{day}/{mes_num}/{year}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy_model(train_data, output_dir=\"modelo_leyes_epigrafe\", n_iter=30, learning_rate=0.001, dropout_rate=0.2):\n",
    "\n",
    "    # Crear un modelo en blanco para español e incorporar el componente de NER\n",
    "    nlp = spacy.blank(\"es\")\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "    # Añadir etiquetas al componente NER basándose en las anotaciones de train_data\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\", []):\n",
    "            ner.add_label(ent[2])  # ent[2] corresponde a la etiqueta\n",
    "\n",
    "    # Mezclar y dividir los datos en entrenamiento (80%) y validación (20%)\n",
    "    random.shuffle(train_data)\n",
    "    split = int(len(train_data) * 0.8)\n",
    "    train_examples_raw = train_data[:split]\n",
    "    dev_examples_raw = train_data[split:]\n",
    "\n",
    "\n",
    "    # Preparar ejemplos de entrenamiento y validación\n",
    "    def prepare_examples(data):\n",
    "        examples = []\n",
    "        for text, annotations in data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            # Se espera que annotations tenga la estructura {\"entities\": [(inicio, fin, etiqueta), ...]}\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "\n",
    "    train_examples = prepare_examples(train_examples_raw)\n",
    "    dev_examples = prepare_examples(dev_examples_raw)\n",
    "\n",
    "    # Inicializar el entrenamiento; si se requiere ajustar la tasa de aprendizaje, \n",
    "    # se podría configurar el optimizador en base a \"learning_rate\" (actualmente no se utiliza)\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for i in range(n_iter):\n",
    "        random.shuffle(train_examples)\n",
    "        losses = {}\n",
    "        # Usar batches de tamaño dinámico\n",
    "        batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            # Actualiza el modelo usando el batch actual, aplicando dropout para evitar overfitting\n",
    "            nlp.update(batch, drop=dropout_rate, losses=losses, sgd=optimizer)\n",
    "        print(f\"Época {i + 1}, pérdidas: {losses}\")\n",
    "\n",
    "        # Evaluar el modelo en el conjunto de validación y mostrar resultados\n",
    "        scores = nlp.evaluate(dev_examples)\n",
    "        print(f\"  Resultados en validación: {scores}\")\n",
    "\n",
    "    # Guardar el modelo entrenado en disco\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(f\"Modelo guardado en: {output_dir}\")\n",
    "\n",
    "    return nlp\n",
    "\n",
    "# Entrenar\n",
    "#nlp_trained = train_spacy_model(TRAIN_DATA, \"modelo_leyes_epigrafe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2353 NUMERO_LEY\n",
      "Fecha final: 17/04/2024\n"
     ]
    }
   ],
   "source": [
    "# nlp_trained = load_spacy_model(\"modelo_leyes_epigrafe\")\n",
    "\n",
    "# test_text = \"2353 17ABR2024 LEY No. ... El Congreso de Colombia,\"\n",
    "# test_text = fix_ocr_issues_for_training(test_text)  # Limpieza\n",
    "# doc = nlp_trained(test_text)\n",
    "# for ent in doc.ents:\n",
    "#     if ent.label_ == \"FECHA\":\n",
    "#         fecha_normalizada = normalize_spacy_date(ent.text)\n",
    "#         print(\"Fecha final:\", fecha_normalizada)\n",
    "#     else:\n",
    "#         print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_metadata(text: str, nlp) -> dict:\n",
    "    # Limpiar el texto antes de procesarlo (se utiliza la función de corrección de OCR)\n",
    "    text_clean = fix_ocr_issues_for_training(text)\n",
    "    doc = nlp(text_clean)\n",
    "    result = {\n",
    "        \"numero_ley\": None,\n",
    "        \"fecha\": None,\n",
    "        \"epigrafe\": None\n",
    "    }\n",
    "    # Recorrer las entidades detectadas y asignarlas al diccionario de resultados\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"NUMERO_LEY\":\n",
    "            result[\"numero_ley\"] = ent.text\n",
    "        elif ent.label_ == \"FECHA\":\n",
    "            # Normalizar la fecha a formato DD/MM/AAAA usando la función correspondiente\n",
    "            fecha_norm = normalize_spacy_date(ent.text)\n",
    "            result[\"fecha\"] = fecha_norm if fecha_norm else ent.text\n",
    "        elif ent.label_ == \"EPIGRAFE\":\n",
    "            result[\"epigrafe\"] = ent.text\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_txt(txt_path: str, nlp=None) -> dict:\n",
    "    \"\"\"\n",
    "    Lee un archivo .txt, extrae metadatos y maneja errores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Archivo no encontrado: {txt_path}\")\n",
    "        return None  # O un diccionario vacío, según prefieras\n",
    "    except UnicodeDecodeError:\n",
    "        logging.error(f\"Error de codificación al leer: {txt_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inesperado al leer {txt_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if nlp:\n",
    "        try:\n",
    "            metadata = spacy_extract_metadata(content, nlp)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al extraer metadatos con spaCy de {txt_path}: {e}\")\n",
    "            metadata = {}  # Diccionario vacío si falla spaCy\n",
    "    else:\n",
    "        metadata = {\n",
    "            \"numero_ley\": None,\n",
    "            \"fecha\": None,\n",
    "            \"tipo_ley\": None,\n",
    "            \"epigrafe\": None\n",
    "        }\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_metadata_from_txt(txt_path: str, nlp=None) -> dict:\n",
    "#     \"\"\"\n",
    "#     Lee un archivo .txt y extrae los metadatos (numero_ley, fecha, etc.)\n",
    "#     usando tu pipeline híbrida o la que definas.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "\n",
    "#     # 1) Abrir y leer el archivo .txt\n",
    "#     with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         content = f.read()\n",
    "\n",
    "#     # 2) Llamar a tu función de extracción (regex + spaCy)\n",
    "#     #    Por ejemplo, si definiste extract_metadata_layout_hybrid:\n",
    "#     if nlp:\n",
    "#         metadata = spacy_extract_metadata(content, nlp)\n",
    "#     else:\n",
    "#         # Si no pasas un nlp, podrías usar solo regex\n",
    "#         # metadata = extract_metadata_layout_regex(content)\n",
    "#         metadata = {\n",
    "#             \"numero_ley\": None,\n",
    "#             \"fecha\": None,\n",
    "#             \"tipo_ley\": None,\n",
    "#             \"epigrafe\": None\n",
    "#         }\n",
    "\n",
    "#     return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the carpet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_txt_en_carpeta(carpeta_txt: str, carpeta_salida: str, nlp=None):\n",
    "    \"\"\"\n",
    "    Procesa archivos .txt, guarda metadatos en JSON y maneja errores.\n",
    "    \"\"\"\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "    for archivo in os.listdir(carpeta_txt):\n",
    "        if archivo.lower().endswith(\".txt\"):\n",
    "            ruta_txt = os.path.join(carpeta_txt, archivo)\n",
    "            metadatos = extract_metadata_from_txt(ruta_txt, nlp)\n",
    "\n",
    "            if metadatos:  # Solo guarda si se extrajeron metadatos\n",
    "                nombre_json = os.path.splitext(archivo)[0] + \".json\"\n",
    "                ruta_json = os.path.join(carpeta_salida, nombre_json)\n",
    "                try:\n",
    "                    with open(ruta_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(metadatos, f, ensure_ascii=False, indent=4)\n",
    "                    logging.info(f\"Metadatos guardados en: {ruta_json}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error al guardar JSON para {archivo}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def procesar_txt_en_carpeta(carpeta_txt: str, carpeta_salida: str, nlp=None):\n",
    "#     \"\"\"\n",
    "#     Procesa todos los archivos .txt en la carpeta 'carpeta_txt',\n",
    "#     extrae los metadatos y los guarda en archivos JSON en 'carpeta_salida'.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import json\n",
    "\n",
    "#     os.makedirs(carpeta_salida, exist_ok=True)  # Crea la carpeta de salida si no existe\n",
    "\n",
    "#     for archivo in os.listdir(carpeta_txt):\n",
    "#         if archivo.lower().endswith(\".txt\"):\n",
    "#             ruta_txt = os.path.join(carpeta_txt, archivo)\n",
    "\n",
    "#             # Llamar a la función que extrae metadatos de un .txt\n",
    "#             metadatos = extract_metadata_from_txt(ruta_txt, nlp)\n",
    "\n",
    "#             # Nombre del archivo JSON\n",
    "#             nombre_json = os.path.splitext(archivo)[0] + \".json\"\n",
    "#             ruta_json = os.path.join(carpeta_salida, nombre_json)\n",
    "\n",
    "#             # Guardar en JSON\n",
    "#             with open(ruta_json, \"w\", encoding=\"utf-8\") as f:\n",
    "#                 json.dump(metadatos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "#             print(f\"Metadatos guardados en: {ruta_json}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Rutas\n",
    "    carpeta_txt = r\"C:\\Users\\Jorge\\OneDrive\\Documents\\proyect\\document\\leyes\"\n",
    "    carpeta_salida = r\"C:\\Users\\Jorge\\OneDrive\\Documents\\proyect\\document\\json_output_2022\"\n",
    "\n",
    "    # 2) Cargar el modelo spaCy (si lo usas)\n",
    "    import spacy\n",
    "    nlp_trained = spacy.load(\"modelo_leyes_epigrafe\")  # o la ruta a tu modelo\n",
    "\n",
    "    # 3) Procesar la carpeta\n",
    "    procesar_txt_en_carpeta(carpeta_txt, carpeta_salida, nlp_trained)\n",
    "\n",
    "# Entrenar\n",
    "nlp_trained = train_spacy_model(TRAIN_DATA, \"modelo_leyes_epigrafe\")\n",
    "\n",
    "#Ejemplo de Uso\n",
    "nlp_trained = load_spacy_model(\"modelo_leyes_epigrafe\")\n",
    "\n",
    "test_text = \"2353 17ABR2024 LEY No. ... El Congreso de Colombia,\"\n",
    "test_text = fix_ocr_issues_for_training(test_text)  # Limpieza\n",
    "doc = nlp_trained(test_text)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"FECHA\":\n",
    "        fecha_normalizada = normalize_spacy_date(ent.text)\n",
    "        print(\"Fecha final:\", fecha_normalizada)\n",
    "    else:\n",
    "        print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
