{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "from typing import Optional\n",
    "from spacy.training import Example\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 1. Funciones de limpieza y normalización (OCR issues)\n",
    "##############################################################################\n",
    "def fix_ocr_issues(text: str) -> str:\n",
    "    correcciones = {\n",
    "        \"NACiÓN\": \"NACIÓN\",\n",
    "        \"Colr,mbiana\": \"Colombiana\",\n",
    "        \"Perscnal\": \"Personal\",\n",
    "        \"AérEa\": \"Aérea\",\n",
    "        \"No1956\": \"No 1956\",\n",
    "        \"No.1956\": \"No. 1956\",\n",
    "        \"N°1956\": \"N° 1956\",\n",
    "        \"LEYNo\": \"LEY No\",\n",
    "        \"LEYNo.\": \"LEY No.\",\n",
    "        \"4JUN2019\": \"4 JUN 2019\",\n",
    "        \"Dadaen\": \"Dada en\",\n",
    "        \"Nc.\": \"No.\",  # Agregamos esta corrección\n",
    "        \"2374'\": \"2374\" #Agregamos esta correccion.\n",
    "    }\n",
    "    for error, correccion in correcciones.items():\n",
    "        text = text.replace(error, correccion)\n",
    "\n",
    "    # Mantener saltos de línea DOBLES, pero eliminar los sencillos DENTRO de párrafos.\n",
    "    text = re.sub(r\"(?<=\\S)\\n(?=\\S)\", \" \", text)  # Junta líneas CON texto\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)        # Máximo 2 saltos seguidos\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)  # Espacios/tabs a 1 espacio\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"(No|N°|No\\.)(\\d+)\", r\"\\1 \\2\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d+)([A-Z]{1,3}\\d{4})\", r\"\\1 \\2\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 2. Regex para fechas, ley, etc. (ejemplo)\n",
    "##############################################################################\n",
    "def parse_date_str(text: str) -> Optional[str]:\n",
    "    patterns = [\n",
    "        r\"(\\d{1,2})\\s*(?:de\\s*)?([A-Z]+)\\s*(?:de\\s*)?(\\d{4})\",\n",
    "        r\"(\\d{1,2})\\s+([A-Z]{3,})\\s+(\\d{4})\",\n",
    "    ]\n",
    "    month_map = {\n",
    "        \"ENE\": \"01\", \"FEB\": \"02\", \"MAR\": \"03\", \"ABR\": \"04\",\n",
    "        \"MAY\": \"05\", \"JUN\": \"06\", \"JUL\": \"07\", \"AGO\": \"08\",\n",
    "        \"SEP\": \"09\", \"OCT\": \"10\", \"NOV\": \"11\", \"DIC\": \"12\",\n",
    "        \"ENERO\": \"01\", \"FEBRERO\": \"02\", \"MARZO\": \"03\", \"ABRIL\": \"04\",\n",
    "        \"MAYO\": \"05\", \"JUNIO\": \"06\", \"JULIO\": \"07\", \"AGOSTO\": \"08\",\n",
    "        \"SEPTIEMBRE\": \"09\", \"OCTUBRE\": \"10\", \"NOVIEMBRE\": \"11\", \"DICIEMBRE\": \"12\"\n",
    "    }\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                day = match.group(1).zfill(2)\n",
    "                month_str = match.group(2).upper()\n",
    "                year = match.group(3)\n",
    "                month = month_map.get(month_str)\n",
    "                if month:\n",
    "                    return f\"{day}/{month}/{year}\"\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ley_and_type(text: str):\n",
    "    type_map = {\n",
    "        \"orgánica\": \"orgánica\",\n",
    "        \"organica\": \"orgánica\",\n",
    "        \"ordinaria\": \"ordinaria\",\n",
    "        \"estatutaria\": \"estatutaria\",\n",
    "        \"estaturaria\": \"estatutaria\",  # Podría haber errores de OCR\n",
    "    }\n",
    "\n",
    "    pattern = r\"\"\"\n",
    "        (?:LEY|LEYES)[\\s\\n]*  # \"LEY\" o \"LEYES\", seguido de espacios/saltos\n",
    "        (?:             # Grupo opcional para el tipo de ley\n",
    "            (ESTATUTARIA|ORDINARIA|ORG[AÁ]NICA)  # Tipos de ley\n",
    "            [\\s\\n]*     # 1 o más espacios/saltos DESPUÉS del tipo\n",
    "        )?              # El tipo de ley es OPCIONAL\n",
    "        (?:N[°ºc]?\\.?|No\\.?)?  # \"N\", \"No\", \"N°\", \"Nº\", \"No.\", \"N.\", \"Nc.\", opcional\n",
    "        [\\s\\n]*         # 0 o más espacios/saltos\n",
    "        (\\d+)          # Captura el número de la ley\n",
    "        (?:['´`‘’])?   # Caracteres especiales opcionales después del número\n",
    "    \"\"\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE | re.VERBOSE | re.DOTALL)\n",
    "\n",
    "    if not match:\n",
    "        return (None, None)  # Si no hay coincidencia, devuelve (None, None)\n",
    "\n",
    "    raw_type = match.group(1)  # Captura el grupo 1 (tipo de ley, si existe)\n",
    "    ley_number = match.group(2)  # Captura el grupo 2 (número de ley)\n",
    "\n",
    "    if raw_type:\n",
    "        tipo_ley = type_map.get(raw_type.lower()) #Uso del diccionario\n",
    "    else:\n",
    "        tipo_ley = \"orgánica\"  # Si no hay tipo explícito, asume \"orgánica\"\n",
    "\n",
    "    return (tipo_ley, ley_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 3. Funciones de validación mínima (opcional)\n",
    "##############################################################################\n",
    "def looks_like_number_ley(value: str) -> bool:\n",
    "    return bool(re.search(r\"\\d+\", value)) if value else False\n",
    "\n",
    "def looks_like_date(candidate: str) -> bool:\n",
    "    return parse_date_str(candidate) is not None\n",
    "\n",
    "def looks_like_epigrafe(text_epigrafe: str) -> bool:\n",
    "    if not text_epigrafe:\n",
    "        return False\n",
    "    if len(text_epigrafe) < 30:\n",
    "        return False\n",
    "    if \"POR MEDIO DE\" not in text_epigrafe.upper():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_section(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Devuelve el contenido desde el inicio del texto\n",
    "    hasta la frase 'EL CONGRESO...' o 'EL CONGRESO DE LA REPUBLICA...'.\n",
    "    Si no la encuentra, devuelve el texto completo.\n",
    "    \"\"\"\n",
    "    pattern = r\"^(.*?)(?=EL\\s+CONGRESO\\s+(?:DE\\s+LA\\s+REPUBLICA|DE\\s+COLOMBIA))\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_section(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Devuelve el contenido desde 'Dada, a los' (o una variante)\n",
    "    hasta el final del texto. Si no lo encuentra, retorna cadena vacía.\n",
    "    \"\"\"\n",
    "    pattern = r\"(?:Dada,\\s+a\\s+los\\s+.*)$\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_date(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extrae la fecha de la sección final del documento (\"Dada, a los...\").\n",
    "    Devuelve la fecha normalizada (DD/MM/YYYY) o None si no la encuentra.\n",
    "    \"\"\"\n",
    "    # Modificación: Ahora solo busca la fecha\n",
    "    pattern = r\"Dada,?\\s+a\\s+los\\s+(.+)\"  # Busca \"Dada, a los\" y captura lo que sigue\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        date_str = match.group(1).strip()  # Obtiene la parte de la fecha\n",
    "        return parse_date_str(date_str)  # Usa la función de parseo para normalizar\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_epigrafe(text: str, fecha_normalizada: str) -> str | None:\n",
    "    if fecha_normalizada:\n",
    "        try:\n",
    "            datetime.strptime(fecha_normalizada, \"%d/%m/%Y\")\n",
    "            start_index = text.find(fecha_normalizada)\n",
    "            if start_index == -1:\n",
    "                start_index = 0\n",
    "                pattern_fecha = r\"(\\d{1,2})\\s*([A-Za-z]{3,})\\s*(\\d{4})\"\n",
    "                match = re.search(pattern_fecha,text)\n",
    "                if match:\n",
    "                  start_index = match.end()\n",
    "            else:\n",
    "                start_index += len(fecha_normalizada)\n",
    "        except ValueError:\n",
    "           start_index = 0\n",
    "           pattern_fecha = r\"(\\d{1,2})\\s*([A-Za-z]{3,})\\s*(\\d{4})\"\n",
    "           match = re.search(pattern_fecha,text)\n",
    "           if match:\n",
    "              start_index = match.end()\n",
    "    else:\n",
    "        ley_match = re.search(r\"(?:LEY|LEYES)[\\s\\n]*(?:N[°ºc]?\\.?|No\\.?)?[\\s\\n]*\\d+\", text, re.IGNORECASE)\n",
    "        if ley_match:\n",
    "            start_index = ley_match.end()\n",
    "            mayuscula_match = re.search(r\"[A-Z]\", text[start_index:])\n",
    "            if mayuscula_match:\n",
    "                start_index += mayuscula_match.start()\n",
    "        else:\n",
    "            start_index = 0\n",
    "\n",
    "    # Intenta buscar \"EL CONGRESO\" (con variaciones) o \"REPUBLICA DE COLOMBIA\"\n",
    "    congreso_match = re.search(r\"EL\\s+CONGRESO|REPUBLICA\\s+DE\\s+COLOMBIA\\s*-\\s*GOBIERNO\\s+NACIONAL\", text, re.IGNORECASE)\n",
    "    if congreso_match:\n",
    "        end_index = congreso_match.start()\n",
    "    else:\n",
    "        end_index = len(text)\n",
    "\n",
    "    if start_index < end_index:\n",
    "        epigrafe = text[start_index:end_index].strip()\n",
    "        # Priorizar la frase \"POR\"\n",
    "        pattern_epigrafe = r\"(POR\\s+(?:MEDIO\\s+DE\\s+LA\\s+CUAL|LA\\s+CUAL)\\s+.*?)(?:\\.\\s*|\\n|EL\\s+CONGRESO|REPUBLICA\\s+DE\\s+COLOMBIA)\"\n",
    "        match_ep = re.search(pattern_epigrafe, epigrafe, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        if match_ep:  #Si encuentra un match con la frase\n",
    "            return match_ep.group(1).strip()\n",
    "        else: #Si no encuentra con la frase, intenta dentro de comillas\n",
    "          pattern_epigrafe = r'[\"“”](.*?)[\"“”]'\n",
    "          match_ep = re.search(pattern_epigrafe, epigrafe, re.DOTALL)\n",
    "          if match_ep:\n",
    "            return match_ep.group(1).strip()\n",
    "          else: #Si no, retorna el texto entre los indices.\n",
    "            return epigrafe\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 5. spaCy: extraer metadatos (NUMERO_LEY, FECHA, EPIGRAFE)\n",
    "##############################################################################\n",
    "def spacy_extract_metadata(text: str, nlp) -> dict:\n",
    "    doc = nlp(text)\n",
    "    result = {\n",
    "        \"numero_ley\": None,\n",
    "        \"fecha\": None,\n",
    "        \"epigrafe\": None\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"NUMERO_LEY\":\n",
    "            result[\"numero_ley\"] = ent.text\n",
    "        elif ent.label_ == \"FECHA\":\n",
    "            result[\"fecha\"] = ent.text\n",
    "        elif ent.label_ == \"EPIGRAFE\":\n",
    "            result[\"epigrafe\"] = ent.text\n",
    "    return result\n",
    "\n",
    "\n",
    "def normalize_spacy_date(spacy_fecha: Optional[str]) -> Optional[str]:\n",
    "    if not spacy_fecha:\n",
    "        return None\n",
    "    parsed = parse_date_str(spacy_fecha)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 6. Función híbrida: extrae metadatos de un texto (regex + spaCy)\n",
    "##############################################################################\n",
    "def extract_metadata_layout_hybrid(text_lineal: str, nlp) -> dict:\n",
    "    # 1) Recortar a la primera sección\n",
    "    first_section = extract_first_section(text_lineal)\n",
    "\n",
    "    # 2) Limpieza OCR, etc.\n",
    "    first_section = fix_ocr_issues(first_section)\n",
    "\n",
    "    # 3) Extraer con spaCy (por si detecta NUMERO_LEY, FECHA, EPIGRAFE)\n",
    "    spacy_result = spacy_extract_metadata(first_section, nlp)\n",
    "\n",
    "    # 4) Regex\n",
    "    tipo_ley, ley_number = parse_ley_and_type(first_section)\n",
    "    fecha_regex = parse_date_str(first_section)\n",
    "\n",
    "    # 5) Normalizar fecha de spaCy\n",
    "    spacy_fecha = spacy_result[\"fecha\"]\n",
    "    spacy_fecha_normalizada = normalize_spacy_date(spacy_fecha)\n",
    "    if spacy_fecha_normalizada:\n",
    "        final_fecha = spacy_fecha_normalizada\n",
    "    else:\n",
    "        final_fecha = fecha_regex\n",
    "\n",
    "    # 6) Epígrafe\n",
    "    epigrafe_regex = extraer_epigrafe(first_section, final_fecha)\n",
    "\n",
    "    # 7) Fallback en número de ley\n",
    "    spacy_numero_ley = spacy_result[\"numero_ley\"]\n",
    "    if not spacy_numero_ley or not looks_like_number_ley(spacy_numero_ley):\n",
    "        spacy_numero_ley = ley_number\n",
    "\n",
    "    # 8) Fallback epígrafe\n",
    "    spacy_epigrafe = spacy_result[\"epigrafe\"]\n",
    "    if spacy_epigrafe:\n",
    "        # si deseas validarlo con looks_like_epigrafe, hazlo\n",
    "        final_epigrafe = spacy_epigrafe\n",
    "    else:\n",
    "        final_epigrafe = epigrafe_regex\n",
    "\n",
    "    return {\n",
    "        \"numero_ley\": spacy_numero_ley,\n",
    "        \"fecha\": final_fecha,\n",
    "        \"tipo_ley\": tipo_ley,\n",
    "        \"epigrafe\": final_epigrafe\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_two_phases(full_text: str, nlp) -> dict:\n",
    "    # 1) Sección de arriba\n",
    "    top_section = extract_first_section(full_text)\n",
    "    top_metadata = extract_metadata_layout_hybrid(top_section, nlp)\n",
    "\n",
    "    # 2) Sección de abajo (buscando 'Dada, a los')\n",
    "    bottom_section = extract_final_section(full_text)\n",
    "    bottom_metadata = extract_metadata_layout_hybrid(bottom_section, nlp)\n",
    "\n",
    "    # 3) Combinar\n",
    "    final_metadata = {\n",
    "        \"numero_ley\": None,\n",
    "        \"fecha\": None,\n",
    "        \"tipo_ley\": None,\n",
    "        \"epigrafe\": None\n",
    "    }\n",
    "\n",
    "    # Priorizar lo que venga en top_metadata\n",
    "    final_metadata[\"numero_ley\"] = top_metadata[\"numero_ley\"] or bottom_metadata[\"numero_ley\"]\n",
    "    final_metadata[\"fecha\"]      = top_metadata[\"fecha\"]      or bottom_metadata[\"fecha\"]\n",
    "    final_metadata[\"tipo_ley\"]   = top_metadata[\"tipo_ley\"]   or bottom_metadata[\"tipo_ley\"]\n",
    "\n",
    "    # Epígrafe: si top_metadata lo trae, usarlo; si no, usar bottom\n",
    "    #Ya no se prioriza el mas largo, sino que se toma el de la seccion inicial.\n",
    "    final_metadata[\"epigrafe\"] = top_metadata[\"epigrafe\"] or bottom_metadata[\"epigrafe\"]\n",
    "\n",
    "    return final_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 7. Función para procesar un archivo .txt completo (como si fuera \"una sola página\")\n",
    "##############################################################################\n",
    "def extract_metadata_from_txt_file(txt_path: str, nlp) -> dict:\n",
    "    # Leer el contenido\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Llamar la función de dos fases\n",
    "    final_metadata = extract_metadata_two_phases(content, nlp)\n",
    "    return final_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 8. Función para procesar TODOS los .txt de una carpeta\n",
    "##############################################################################\n",
    "def procesar_txt_en_carpeta(carpeta_txt, carpeta_salida, nlp):\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "    for archivo in os.listdir(carpeta_txt):\n",
    "        if archivo.lower().endswith(\".txt\"):\n",
    "            ruta_txt = os.path.join(carpeta_txt, archivo)\n",
    "\n",
    "            metadatos = extract_metadata_from_txt_file(ruta_txt, nlp)\n",
    "\n",
    "            # Guardar JSON\n",
    "            nombre_json = os.path.splitext(archivo)[0] + \".json\"\n",
    "            ruta_json = os.path.join(carpeta_salida, nombre_json)\n",
    "            with open(ruta_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(metadatos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "            print(f\"Procesado {archivo} -> Metadatos en: {ruta_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 9. (Opcional) Entrenar y cargar modelo spaCy\n",
    "##############################################################################\n",
    "def train_spacy_model(train_data, output_dir=\"modelo_leyes_epigrafe\"):\n",
    "    nlp = spacy.blank(\"es\")\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Añadir etiquetas\n",
    "    for text, annotations in train_data:\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            ner.add_label(label)\n",
    "\n",
    "    nlp.initialize()\n",
    "    examples = []\n",
    "    for text, annotations in train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "\n",
    "    for i in range(10):\n",
    "        random.shuffle(examples)\n",
    "        losses = {}\n",
    "        for example in examples:\n",
    "            nlp.update([example], losses=losses)\n",
    "        print(f\"Época {i}, pérdidas: {losses}\")\n",
    "\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(f\"Modelo guardado en: {output_dir}\")\n",
    "    return nlp\n",
    "\n",
    "def load_spacy_model(model_path=\"modelo_leyes_epigrafe\"):\n",
    "    return spacy.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado LEY-2350-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2350-2024plaintext.json\n",
      "Procesado LEY-2351-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2351-2024plaintext.json\n",
      "Procesado LEY-2352-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2352-2024plaintext.json\n",
      "Procesado LEY-2353-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2353-2024plaintext.json\n",
      "Procesado LEY-2354-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2354-2024plaintext.json\n",
      "Procesado LEY-2355-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2355-2024plaintext.json\n",
      "Procesado LEY-2356-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2356-2024plaintext.json\n",
      "Procesado LEY-2358-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2358-2024plaintext.json\n",
      "Procesado LEY-2359-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2359-2024plaintext.json\n",
      "Procesado LEY-2360-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2360-2024plaintext.json\n",
      "Procesado LEY-2361-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2361-2024plaintext.json\n",
      "Procesado LEY-2364-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2364-2024plaintext.json\n",
      "Procesado LEY-2365-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2365-2024plaintext.json\n",
      "Procesado LEY-2366-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2366-2024plaintext.json\n",
      "Procesado LEY-2367-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2367-2024plaintext.json\n",
      "Procesado LEY-2368-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2368-2024plaintext.json\n",
      "Procesado LEY-2369-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2369-2024plaintext.json\n",
      "Procesado LEY-2373-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2373-2024plaintext.json\n",
      "Procesado LEY-2374-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2374-2024plaintext.json\n",
      "Procesado LEY-2375-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2375-2024plaintext.json\n",
      "Procesado LEY-2376-2024plaintext.txt -> Metadatos en: c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\\LEY-2376-2024plaintext.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Rutas de carpeta\n",
    "    carpeta_txt = r\"c:/Users/Jorge/OneDrive/Documents/proyect/document/leyes\"\n",
    "    carpeta_salida = r\"c:/Users/Jorge/OneDrive/Documents/proyect/document/json_output_2024\"\n",
    "\n",
    "    # Cargar tu modelo spaCy (si ya lo tienes entrenado)\n",
    "    nlp_trained = load_spacy_model(\"modelo_leyes_epigrafe\")  # ejemplo\n",
    "    # O si estás usando un EntityRuler, define nlp_trained antes.\n",
    "\n",
    "    # Procesar todos los .txt de la carpeta\n",
    "    procesar_txt_en_carpeta(carpeta_txt, carpeta_salida, nlp_trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
